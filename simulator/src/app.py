import os
import shutil
import json
import math
from datetime import datetime, timedelta
from threading import Thread

from flask import Flask, jsonify, Response, request
from flask_sqlalchemy import SQLAlchemy
from sqlalchemy import text, desc
from werkzeug.exceptions import BadRequest

import constants
from config_parser import parse_config, get_source_code_dir
from open_search_simulator import Simulator
from cluster_dynamic import ClusterDynamic
from plotter import plot_data_points
from open_search_simulator import timeit


app = Flask(__name__)
app.config["SQLALCHEMY_DATABASE_URI"] = "sqlite:///datapoints.db"
app.app_context().push()
if os.path.exists("instance"):
    shutil.rmtree("instance")
db = SQLAlchemy(app)


# Database model to store the datapoints
class DataModel(db.Model):
    status = db.Column(db.String(200))
    cpu_usage_percent = db.Column(db.Float, default=0)
    memory_usage_percent = db.Column(db.Float, default=0)
    heap_usage_percent = db.Column(db.Float, default=0)
    shards_count = db.Column(db.Integer, default=0)
    total_nodes_count = db.Column(db.Integer, default=0)
    active_shards_count = db.Column(db.Integer, default=0)
    active_primary_shards = db.Column(db.Integer, default=0)
    initializing_shards_count = db.Column(db.Integer, default=0)
    unassigned_shards_count = db.Column(db.Integer, default=0)
    relocating_shards_count = db.Column(db.Integer, default=0)
    master_eligible_nodes_count = db.Column(db.Integer, default=0)
    active_data_nodes = db.Column(db.Integer, default=0)
    date_created = db.Column(db.DateTime, default=datetime.now(), primary_key=True)
    disk_usage_percent = db.Column(db.Integer, default=0)
    rolled_index_size = db.Column(db.Float, default=0)


def get_provision_status():
    """
    Returns the status of provision.
    """
    return is_provisioning


def set_provision_status(provisioning):
    """
    Sets the provision status.
    :param provisioning: boolean value that indicates provision status.
    """
    global is_provisioning
    is_provisioning = provisioning


def get_accelerate_flag():
    """
    Function fetches the accelerate flag value.

    :return : bool value that indicates status of accelerate flag.
    """
    return accelerate_flag


def set_accelerate_flag(is_accelerated):
    """
    Function sets a flag which indicates the accelertion of time
    taken for shard rebalancing.

    :param is_accelerated: Boolean value used to set the accelerate flag.
    """
    global accelerate_flag
    accelerate_flag = is_accelerated


def get_first_data_point_time():
    """
    Function queries the database for the time corresponding to first data point
    generated by the simulator

    :return: Time corresponding to first data point generated by the simulator
    """
    first_data_point_time = (
        DataModel.query.order_by(DataModel.date_created)
        .with_entities(DataModel.date_created)
        .first()
    )
    return first_data_point_time[0]


def cluster_db_object(cluster):
    """
    Create a DataModel instance that can be dumped into db.

    :param cluster: cluster object.
    :return: An object of class DataModel.
    """
    return DataModel(
        cpu_usage_percent=cluster.cpu_usage_percent,
        memory_usage_percent=cluster.memory_usage_percent,
        heap_usage_percent=cluster.heap_usage_percent,
        date_created=cluster.date_time,
        status=cluster.status,
        total_nodes_count=cluster.total_nodes_count,
        active_shards_count=cluster.active_shards,
        active_primary_shards=cluster.active_primary_shards,
        initializing_shards_count=cluster.initializing_shards,
        unassigned_shards_count=cluster.unassigned_shards,
        relocating_shards_count=cluster.relocating_shards,
        master_eligible_nodes_count=cluster.master_eligible_nodes_count,
        active_data_nodes=cluster.active_data_nodes,
        disk_usage_percent=cluster.disk_usage_percent,
        rolled_index_size=cluster.rolled_index_size,
    )


def overwrite_after_node_count_change(cluster_objects, time=None):
    """
    Calculate the resource utilization after node change operation
    and overwrite the saved data points in db after node change time.
    Also create an overlap on the png file to show new data points.

    :param cluster_objects: all cluster objects with new node configuration.
    :param date_time: date time object to overwrite date time now.
    :return: expiry time.
    """
    if time == None:
        date_time = datetime.now()
    else:
        date_time = time

    cluster_objects_post_change = []
    for cluster_obj in cluster_objects:
        if cluster_obj.date_time >= date_time:
            cluster_objects_post_change.append(cluster_obj)
            task = cluster_db_object(cluster_obj)
            db.session.merge(task)

    db.session.commit()
    plot_data_points(
        cluster_objects_post_change, skip_data_ingestion=True, skip_search_query=True
    )
    return


@timeit
def reset_load(sim, time=None):
    """
    The fuction resets shard size from end of simulation
    to shard size configured at current time.

    :param sim: An object of class Simulator.
    :param time: Optional time parameter to set shard configuration to specified time.
    """
    sim.cluster.clear_index_size()

    if time == None:
        now = datetime.now()
    else:
        now = time

    current_disk = (
        DataModel.query.order_by(desc(DataModel.date_created))
        .filter(DataModel.date_created <= now)
        .with_entities(
            DataModel.__getattribute__(DataModel, constants.STAT_REQUEST["DiskUtil"])
        )
        .first()
    )

    current_rolled_size = (
        DataModel.query.order_by(desc(DataModel.date_created))
        .filter(DataModel.date_created <= now)
        .with_entities(DataModel.__getattribute__(DataModel, "rolled_index_size"))
        .first()
    )

    rolled_index_size = current_rolled_size[0]
    distribution_size = (current_disk[0] / 100) * sim.cluster.total_disk_size_gb
    distribution_size -= rolled_index_size
    shard_size = distribution_size / sim.cluster.total_shard_count
    distribution_size -= (
        sim.cluster.replica_shards_per_index
        * sim.cluster.primary_shards_per_index
        * sim.cluster.index_count
        * (shard_size)
    )
    sim.cluster.rolled_index_size = rolled_index_size
    sim.cluster.indices[sim.cluster.rolled_over_index_id].shards[
        0
    ].shard_size = rolled_index_size
    sim.cluster.indices[sim.cluster.rolled_over_index_id].index_size = rolled_index_size
    sim.distribute_load((distribution_size / sim.frequency_minutes) * 60)


def reset_current_index_count(duration, sim, time):
    """
    Resets the index configuration from end of simulation to current time.

    :param duration: Time (in minutes) to be re-simulated.
    :param sim: An object of class Simulator.
    :param time: Time corresponding to which index configuration will be reset.
    """
    if time == None:
        time = datetime.now()

    start_time_simulate = sim.total_simulation_minutes - duration
    start_time_minute = int(
        (start_time_simulate - (start_time_simulate % sim.frequency_minutes))
        / sim.frequency_minutes
    )
    sim.current_index_count = sim.index_added_list[start_time_minute]

    for index in range(len(sim.cluster.indices[: sim.current_index_count])):
        total_hours_elapsed = (
            (time - sim.cluster.indices[index].created_at).total_seconds()
        ) // 3600
        sim.cluster.indices[index].time_elapsed_last_roll_over = time - timedelta(
            hours=int(total_hours_elapsed % sim.cluster.index_roll_over_hours)
        )


def get_duration_for_resimulation(time):
    """
    Fetches duration in minutes to re-simulate.

    :param time: Time corresponding to which duration of re-simulation must be evaluated.
    :return: Duration (in minutes) for resimulation.
    """
    app.app_context().push()

    if time == None:
        time_now = datetime.now()
    else:
        time_now = time

    simulation_end_date_time = (
        DataModel.query.order_by(desc(DataModel.date_created))
        .with_entities(DataModel.__getattribute__(DataModel, "date_created"))
        .first()
    )
    resimulation_time = math.ceil(
        ((simulation_end_date_time[0] - time_now).total_seconds()) / 60
    )

    return int(resimulation_time) + 5


def get_simulated_points():
    """
    Returns simulated data rate, search rate and
    total duration in minutes from the first simulation
    """
    data_rate = sim.simulated_data_rates.copy()
    search_rate = sim.simulated_search_rates.copy()
    index_added = sim.index_added_list.copy()
    total_minutes = sim.total_simulation_minutes

    return data_rate, search_rate, total_minutes, index_added


@timeit
def add_node_and_rebalance(nodes, time=None):
    """
    Increments node count in cluster object and rebalances shards
    among available nodes. Re-Simulates data after node addition
    and shard rebalance.

    :param nodes: count of node(s) to be added to cluster
    :param time: Optional parameter which performs node addtion to corresponding time.
    """
    app.app_context().push()

    data_rate, search_rate, total_minutes, index_count = get_simulated_points()
    sim = Simulator(
        configs.cluster,
        configs.data_function,
        configs.search_description,
        configs.searches,
        configs.simulation_frequency_minutes,
        configs.index_addition,
    )
    sim.simulated_data_rates = data_rate
    sim.simulated_search_rates = search_rate
    sim.total_simulation_minutes = total_minutes
    sim.index_added_list = index_count

    duration = get_duration_for_resimulation(time)

    if time == None:
        hour = datetime.now().hour
        minutes = (
            str(datetime.now().minute)
            if datetime.now().minute > 9
            else "0" + str(datetime.now().minute)
        )
    else:
        hour = time.hour
        minutes = (
            str(time.now().minute)
            if time.now().minute > 9
            else "0" + str(time.now().minute)
        )

    reset_current_index_count(duration, sim, time)
    reset_load(sim, time)

    is_accelerated = get_accelerate_flag()
    sim.cluster.add_nodes(nodes, is_accelerated)
    set_accelerate_flag(False)

    cluster_objects = sim.run(duration, str(hour) + "_" + minutes + "_00", True, time)
    overwrite_after_node_count_change(cluster_objects, time)

    is_provisioning = get_provision_status()
    is_provisioning = False
    set_provision_status(is_provisioning)


@timeit
def rem_node_and_rebalance(nodes, time=None):
    """
    Decrements node count in cluster object and rebalances the shards
    among the available nodes. Re-Simulates the data once the node is
    removed and shards are distributed

    :param nodes: count of node(s) to be removed from cluster.
    :param time: Optional parameter which performs node removal to corresponding time.
    """
    app.app_context().push()

    data_rate, search_rate, total_minutes, index_count = get_simulated_points()
    sim = Simulator(
        configs.cluster,
        configs.data_function,
        configs.search_description,
        configs.searches,
        configs.simulation_frequency_minutes,
        configs.index_addition,
    )
    sim.simulated_data_rates = data_rate
    sim.simulated_search_rates = search_rate
    sim.total_simulation_minutes = total_minutes
    sim.index_added_list = index_count

    duration = get_duration_for_resimulation(time)

    if time == None:
        hour = datetime.now().hour
        minutes = (
            str(datetime.now().minute)
            if datetime.now().minute > 9
            else "0" + str(datetime.now().minute)
        )
    else:
        hour = time.hour
        minutes = (
            str(time.now().minute)
            if time.now().minute > 9
            else "0" + str(time.now().minute)
        )

    reset_current_index_count(duration, sim, time)
    reset_load(sim, time)

    is_accelerated = get_accelerate_flag()
    sim.cluster.remove_nodes(nodes, is_accelerated)
    set_accelerate_flag(False)

    cluster_objects = sim.run(duration, str(hour) + "_" + minutes + "_00", True, time)
    overwrite_after_node_count_change(cluster_objects, time)

    is_provisioning = get_provision_status()
    is_provisioning = False
    set_provision_status(is_provisioning)


@app.route("/stats/violated")
def violated_count():
    """
    Endpoint fetches the violated count for a requested metric, threshold and duration,
    The metric,duration and threshold will be sent as query parameter.
    :param metric: represents the metric that is being queried.
    :param duration: represents the time period for fetching the average
    :param threshold: represents the limit considered for evaluating violated count
    :return: count of stat exceeding the threshold for a given duration
    """
    args = request.args
    args.to_dict()

    metric = args.get(constants.METRIC_PARAMETER, type=str)
    duration = args.get(constants.DURATION_PARAMETER, type=int)
    threshold = args.get(constants.THRESHOLD_PARAMETER, type=float)
    time_now_arg = args.get(constants.TIME_NOW_PARAMETER, type=str)

    err_string = ''

    if not metric:
        err_string += f'Expected Query Parameter - "{constants.METRIC_PARAMETER}" '
    if not duration:
        err_string += f'Expected Query Parameter - "{constants.DURATION_PARAMETER}" '
    if not threshold:
        err_string += f'Expected Query Parameter - "{constants.THRESHOLD_PARAMETER}" '
    if len(args) > constants.QUERY_ARG_LENGTH_FOUR:
        err_string += f'Expected Query Parameter Count: {constants.QUERY_ARG_LENGTH_FOUR}, passed: {len(args)} '
    if len(args) == constants.QUERY_ARG_LENGTH_FOUR and time_now_arg == None:
        err_string += f'Expected "{constants.TIME_NOW_PARAMETER}" query parameter '

    if err_string:
        return Response(json.dumps(err_string), status=400)

    if time_now_arg:
        try:
            time_now = datetime.strptime(time_now_arg, constants.TIME_FORMAT)
        except:
            return Response(json.dumps("Invalid query parameters"), status=400)
    else:
        time_now = datetime.now()

    # Convert the minutes to time object to compare and query for required data points
    query_begin_time = time_now - timedelta(minutes=duration)
    first_data_point_time = get_first_data_point_time()
    try:
        # Fetching the count of data points for given duration.
        data_point_count = (
            DataModel.query.order_by(constants.STAT_REQUEST[metric])
            .filter(DataModel.date_created > query_begin_time)
            .filter(DataModel.date_created <= time_now)
            .count()
        )

        # If expected data points are not present then respond with error
        if first_data_point_time > query_begin_time:
            return Response(json.dumps("Not enough Data points"), status=400)

        # Fetches the count of stat_name that exceeds the threshold for given duration
        stats = (
            DataModel.query.order_by(constants.STAT_REQUEST[metric])
            .filter(
                DataModel.__getattribute__(DataModel, constants.STAT_REQUEST[metric])
                > threshold
            )
            .filter(DataModel.date_created > query_begin_time)
            .filter(DataModel.date_created < time_now)
            .count()
        )

        return jsonify({"ViolatedCount": stats})

    except KeyError:
        return Response(f"stat not found - {metric}", status=404)
    except Exception as e:
        return Response(e, status=404)


@app.route("/stats/avg", methods=["GET"])
def average():
    """
    The endpoint evaluates average of requested stat for a duration
    returns error if sufficient data points are not present.
    The metric and duration will be sent as query parameter.
    :param metric: represents the stat that is being queried.
    :param duration: represents the time period for fetching the average
    :return: average of the provided metric for the decision period.
    """
    args = request.args
    args.to_dict()
    metric = args.get(constants.METRIC_PARAMETER, type=str)
    duration = args.get(constants.DURATION_PARAMETER, type=int)
    time_now_arg = args.get(constants.TIME_NOW_PARAMETER, type=str)

    err_string = ''

    if not metric:
        err_string += f'Expected Query Parameter - "{constants.METRIC_PARAMETER}" '
    if not duration:
        err_string += f'Expected Query Parameter - "{constants.DURATION_PARAMETER}" '
    if len(args) > constants.QUERY_ARG_LENGTH_TWO and not time_now_arg:
        err_string += f'Expected "{constants.TIME_NOW_PARAMETER}" query parameter '
    if len(args) > constants.QUERY_ARG_LENGTH_THREE:
        err_string += f'Expected Query Parameter Count: {constants.QUERY_ARG_LENGTH_THREE}, passed: {len(args)} '
    if err_string:
        return Response(json.dumps(err_string), status=400)

    # calculate time to query for data
    if time_now_arg:
        try:
            time_now = datetime.strptime(time_now_arg, constants.TIME_FORMAT)
        except:
            return Response(json.dumps('Invalid value passed in query parameter "time_now"'), status=400)
    else:
        time_now = datetime.now()

    # Convert the minutes to time object to compare and query for required data points
    query_begin_time = time_now - timedelta(minutes=duration)
    first_data_point_time = get_first_data_point_time()
    stat_list = []
    try:
        # Fetches list of rows that is filter by stat_name and are filtered by decision period
        avg_list = (
            DataModel.query.order_by(constants.STAT_REQUEST[metric])
            .filter(DataModel.date_created > query_begin_time)
            .filter(DataModel.date_created <= time_now)
            .with_entities(text(constants.STAT_REQUEST[metric]))
            .all()
        )
        for avg_value in avg_list:
            stat_list.append(avg_value[0])

        # If expected data points count are not present then respond with error
        if first_data_point_time > query_begin_time:
            return Response(json.dumps("Not enough Data points"), status=400)

        # check if any data points were collected
        if not stat_list:
            return Response(json.dumps("Not enough Data points"), status=400)

        # Average, minimum and maximum value of a stat for a given decision period
        return jsonify(
            {
                "avg": sum(stat_list) / len(stat_list),
                "min": min(stat_list),
                "max": max(stat_list),
            }
        )

    except KeyError:
        return Response(f"stat not found - {metric}", status=404)
    except Exception as e:
        return Response(e, status=404)


@app.route("/stats/current", methods=["GET"])
def current_all():
    """
    The endpoint returns all the stats from the latest poll,
    Returns error if sufficient data points are not present.
    """
    args = request.args
    args.to_dict()
    metric = args.get(constants.METRIC_PARAMETER, type=str)
    time_now_arg = args.get(constants.TIME_NOW_PARAMETER, type=str)

    err_string = ''

    if len(args) > constants.QUERY_ARG_LENGTH_TWO:
        err_string += f'Expected Query Parameter Count: {constants.QUERY_ARG_LENGTH_TWO}, passed: {len(args)} '

    if len(args) == constants.QUERY_ARG_LENGTH_TWO and (
        time_now_arg == None or metric == None
    ): 
        err_string += f'Expected "{constants.METRIC_PARAMETER}" and/or {constants.TIME_NOW_PARAMETER} '

    if len(args) == constants.QUERY_ARG_LENGTH_ONE and (
        time_now_arg == None and metric == None
    ):
        err_string += f'Expected "{constants.METRIC_PARAMETER}", passed "{constants.TIME_NOW_PARAMETER}"'
        return Response(json.dumps("Invalid query parameters"), status=400)

    if time_now_arg:
        try:
            time_now = datetime.strptime(time_now_arg, constants.TIME_FORMAT)
        except:
            return Response(json.dumps("Invalid query parameters"), status=400)
    else:
        time_now = datetime.now()

    if metric != None:
        try:
            if constants.STAT_REQUEST[metric] == constants.CLUSTER_STATE:
                # is_provisioning = get_provision_status()
                if get_provision_status():
                    # if Simulator.is_provision_in_progress():
                    return jsonify({"current": constants.CLUSTER_STATE_YELLOW})
            # Fetches the stat_name for the latest poll
            current_stat = (
                DataModel.query.order_by(desc(DataModel.date_created))
                .with_entities(
                    DataModel.__getattribute__(
                        DataModel, constants.STAT_REQUEST[metric]
                    )
                )
                .filter(DataModel.date_created <= time_now)
                .all()
            )

            # If expected data points count are not present then respond with error
            if len(current_stat) == 0:
                return Response(json.dumps("Not enough Data points"), status=400)

            return jsonify({"current": current_stat[0][constants.STAT_REQUEST[metric]]})

        except KeyError:
            return Response(f"stat not found - {metric}", status=404)
        except Exception as e:
            return Response(e, status=404)

    is_provisioning = get_provision_status()
    if is_provisioning:
        return jsonify(cluster_dynamic.__dict__)
    try:
        stat_dict = {}
        for key in constants.STAT_REQUEST_CURRENT:
            value = (
                DataModel.query.order_by(desc(DataModel.date_created))
                .with_entities(
                    DataModel.__getattribute__(
                        DataModel, constants.STAT_REQUEST_CURRENT[key]
                    )
                )
                .filter(DataModel.date_created <= time_now)
                .all()
            )
            stat_dict[key] = value[0][0]
        return jsonify(stat_dict)

    except Exception as e:
        return Response(str(e), status=404)


@app.route("/provision/addnode", methods=["POST"])
def add_node():
    """
    Endpoint to simulate that a node is being added to the cluster
    Expects request body to specify the number of nodes added
    :return: total number of resultant nodes and duration of cluster state as yellow
    """
    is_provisioning = get_provision_status()
    if is_provisioning:
        return Response(
            json.dumps(
                "Cannot perform requested operation as Provisioning is in progress"
            ),
            status=404,
        )
    is_provisioning = True
    set_provision_status(is_provisioning)

    try:
        nodes = int(request.json["nodes"])
        node_count = sim.cluster.total_nodes_count + nodes
    except BadRequest as err:
        is_provisioning = False
        set_provision_status(is_provisioning)
        return Response(json.dumps("expected key 'nodes'"), status=404)

    try:
        time_now_arg = request.json["time_now"]
        time_now = datetime.strptime(time_now_arg, constants.TIME_FORMAT)
        set_accelerate_flag(True)
    except:
        time_now = None

    add_node_thread = Thread(target=add_node_and_rebalance, args=(nodes, time_now))
    add_node_thread.start()
    return jsonify({"nodes": node_count})


@app.route("/provision/remnode", methods=["POST"])
def remove_node():
    """
    Endpoint to simulate that a node is being removed from the cluster
    Expects request body to specify the number of nodes added
    :return: total number of resultant nodes and duration of cluster state as yellow
    """
    is_provisioning = get_provision_status()
    if is_provisioning:
        return Response(
            json.dumps(
                "Cannot perform requested operation as Provisioning is in progress"
            ),
            status=404,
        )
    is_provisioning = True
    set_provision_status(is_provisioning)

    try:
        nodes = int(request.json["nodes"])
        node_count = sim.cluster.total_nodes_count - nodes
        if sim.cluster.total_nodes_count - nodes < sim.cluster.min_nodes_in_cluster:
            return Response(
                json.dumps(
                    "Cannot remove more node(s), Minimum nodes required: ",
                    sim.cluster.min_nodes_in_cluster,
                ),
                status=404,
            )
    except BadRequest as err:
        is_provisioning = False
        set_provision_status(is_provisioning)
        return Response(json.dumps("expected key 'nodes'"), status=404)

    try:
        time_now_arg = request.json["time_now"]
        time_now = datetime.strptime(time_now_arg, constants.TIME_FORMAT)
        set_accelerate_flag(True)
    except:
        time_now = None

    rem_node_thread = Thread(target=rem_node_and_rebalance, args=(nodes, time_now))
    rem_node_thread.start()
    return jsonify({"nodes": node_count})


@app.route("/all")
def all_data():
    """
    Endpoint to fetch the count of datapoints generated by simulator
    :return: returns the count of total datapoints generated by the simulator
    """
    count = DataModel.query.with_entities(
        DataModel.cpu_usage_percent, DataModel.memory_usage_percent, DataModel.status
    ).count()
    return jsonify(count)


if __name__ == "__main__":
    db.create_all()
    cluster_dynamic = ClusterDynamic()
    is_provisioning = False
    accelerate_flag = False

    # get configs from config yaml
    configs = parse_config(
        os.path.join(get_source_code_dir(), constants.CONFIG_FILE_PATH)
    )

    # create the simulator object
    sim = Simulator(
        configs.cluster,
        configs.data_function,
        configs.search_description,
        configs.searches,
        configs.simulation_frequency_minutes,
        configs.index_addition,
    )
    sim.cluster.cluster_dynamic = cluster_dynamic

    days = len(sim.data_ingestion.states)
    cluster_objects = sim.run(24 * 60 * days)

    # save the generated data points to png
    plot_data_points(cluster_objects)

    # save data points inside db
    for cluster_obj in cluster_objects:
        task = cluster_db_object(cluster_obj)
        db.session.add(task)
    db.session.commit()

    # start serving the apis
    app.run(port=constants.APP_PORT)
